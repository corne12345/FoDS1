{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that first rewrites certain patterns in a text to valid words and\n",
    "# then tokenizes all the words the text.\n",
    "def tweet_cleaner_updated(text):\n",
    "    tok = WordPunctTokenizer()\n",
    "\n",
    "    # Regexes used to rewrite certain patterns to valid words.\n",
    "    pat1 = r'@[A-Za-z0-9_]+|#[A-Za-z0-9_]+'\n",
    "    pat2 = r'https?://[^ ]+'\n",
    "    combined_pat = r'|'.join((pat1, pat2))\n",
    "    www_pat = r'www.[^ ]+'\n",
    "    negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                    \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                    \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                    \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                    \"mustn't\":\"must not\"}\n",
    "    neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "    # Tokenize words.\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip().split(' ')\n",
    "\n",
    "def tweet_cleaner_updated_selection(text):\n",
    "    tok = WordPunctTokenizer()\n",
    "\n",
    "    # Regexes used to rewrite certain patterns to valid words.\n",
    "    pat2 = r'https?://[^ ]+'\n",
    "    www_pat = r'www.[^ ]+'\n",
    "    negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                    \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                    \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                    \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                    \"mustn't\":\"must not\"}\n",
    "    neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "    # Tokenize words.\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(pat2, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip().split(' ')\n",
    "\n",
    "def stemmer(df):\n",
    "    ps = PorterStemmer()\n",
    "    sentenceList = df['text_clean'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() ]))\n",
    "    wordsList = sentenceList.apply(lambda x: x.split())\n",
    "    df['text_stemmed'] = wordsList\n",
    "\n",
    "def lemmer(df):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    sentenceList = df['text_clean'].apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))\n",
    "    wordsList = sentenceList.apply(lambda x: x.split())\n",
    "    df['text_lemmed'] = wordsList\n",
    "\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "def word_feats_tweet(strng):\n",
    "    temp = tweet_cleaner_updated(strng)\n",
    "    return word_feats(temp)\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_training(file):\n",
    "    sentiment_training = pd.read_csv(file)[['sentiment', 'text']]\n",
    "    sent_pos = sentiment_training[sentiment_training['sentiment'] == 4]['text']\n",
    "    sent_neg = sentiment_training[sentiment_training['sentiment'] == 0]['text']\n",
    "    sent_pos_clean = list(sent_pos.apply(tweet_cleaner_updated))\n",
    "    sent_neg_clean = list(sent_neg.apply(tweet_cleaner_updated))\n",
    "    negfeats = [(word_feats(sent_neg_clean[i]), 'neg') for i in range(len(sent_neg_clean))]\n",
    "    posfeats = [(word_feats(sent_pos_clean[i]), 'pos') for i in range(len(sent_pos_clean))]\n",
    "    negcutoff = int(len(negfeats)*0.75)\n",
    "    poscutoff = int(len(posfeats)*0.75)\n",
    "    \n",
    "    # Construct the training dataset containing 50% positive reviews and 50% negative reviews\n",
    "    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "\n",
    "    # Construct the negative dataset containing 50% positive reviews and 50% negative reviews\n",
    "    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "    print ('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "    # Train a NaiveBayesClassifier\n",
    "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "\n",
    "    # Test the trained classifier and display the most informative features.\n",
    "    print ('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "    classifier.show_most_informative_features()\n",
    "    \n",
    "    return classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 8655 instances, test on 2886 instances\n",
      "accuracy: 0.8801108801108801\n",
      "Most Informative Features\n",
      "                    rock = True              pos : neg    =     39.6 : 1.0\n",
      "                favorite = True              pos : neg    =     34.9 : 1.0\n",
      "                passbook = True              pos : neg    =     34.9 : 1.0\n",
      "                 worries = True              pos : neg    =     32.4 : 1.0\n",
      "               beautiful = True              pos : neg    =     27.2 : 1.0\n",
      "                   raise = True              pos : neg    =     24.6 : 1.0\n",
      "                  prompt = True              pos : neg    =     24.6 : 1.0\n",
      "                 awesome = True              pos : neg    =     21.4 : 1.0\n",
      "                     hrs = True              neg : pos    =     21.0 : 1.0\n",
      "                    hold = True              neg : pos    =     20.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_training('sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is able to train a model based on the airline data with an accuracy of nearly 0.9. This is promising, but doesn't provide any garanties for other types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model on US Election data\n",
    "The next step is to use the newly trained classifier on the actual dataset. As a first step, the whole dataset is split into tweets mentioning Hillary, Trump or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returns dict with tweets from file: \"fileName\".\n",
    "def loadTweets(fileName = 'geotagged_tweets_20160812-0912.jsons', length = 9999999):\n",
    "    counter = 0\n",
    "    tweets_dict = {}\n",
    "    with open('geotagged_tweets_20160812-0912.jsons') as fd:\n",
    "        for line in tqdm_notebook(fd):\n",
    "            j_content = json.loads(line)\n",
    "            text = j_content['text']\n",
    "            try:\n",
    "                country = j_content['place']['country_code']\n",
    "                location = j_content['place']['full_name']\n",
    "            except:\n",
    "                pass\n",
    "            timestamp = j_content['created_at']\n",
    "            tweets_dict[counter] = [timestamp, location, country, text]\n",
    "            counter += 1\n",
    "            if counter == length: # to keep it  small\n",
    "                break\n",
    "    return pd.DataFrame.from_dict(tweets_dict, orient='index').rename(columns={0:'timestamp', 1:'location', 2:'country', 3: 'text'})\n",
    "\n",
    "def tweets_to_candidates(df):\n",
    "    \"\"\"\n",
    "    This function takes a dataframe of tweets as input.\n",
    "    It select entries that react to either Trump, Hillary or both.\n",
    "    It then places the tweet text of the  entries in the corresponding dictionary.\n",
    "    This is the tweet text as a list entry with the tweeters state as key.\n",
    "    The output is a dict with 3 dicts (both, trump, hillary) with all states as keys and the tweets from that state as valeus.\n",
    "\n",
    "    \"\"\"\n",
    "    both_df = pd.DataFrame(columns=df.columns)\n",
    "    trump_df = pd.DataFrame(columns=df.columns)\n",
    "    hillary_df = pd.DataFrame(columns=df.columns)\n",
    "    df['text'] = df['text'].apply(tweet_cleaner_updated_selection)\n",
    "    for i in tqdm_notebook(range(df.shape[0])):\n",
    "        if df.iloc[i]['country'] == 'US':\n",
    "            if any(s in df['text'].iloc[i] for s in ['realdonaldtrump', 'trump', 'donald', 'donaldtrump'])\\\n",
    "            and any(s in df['text'].iloc[i] for s in['hillaryclinton','hillary','clinton']):\n",
    "                both_df = both_df.append(df.iloc[i], ignore_index=True)\n",
    "            elif any(s in df['text'].iloc[i] for s in ['realdonaldtrump', 'trump', 'donald', 'donaldtrump']):\n",
    "                trump_df = trump_df.append(df.iloc[i], ignore_index=True)\n",
    "            elif any(s in df['text'].iloc[i] for s in['hillaryclinton','hillary','clinton']):\n",
    "                hillary_df = hillary_df.append(df.iloc[i], ignore_index=True)\n",
    "            else:\n",
    "#                 print(df.iloc[i]['text'])\n",
    "                pass\n",
    "    return both_df, trump_df, hillary_df\n",
    "\n",
    "def score_per_selection(df):\n",
    "    \"\"\"\n",
    "    Function that gives a score of the sentiment of a selection of tweets\n",
    "    Positive scores are indicators of postive sentiment, and negative scores for negative sentiment.\n",
    "    The returned score is the average score, so total sentiment/#tweets\n",
    "    \"\"\"\n",
    "    tweets_lst = list(df['text'])\n",
    "    test_features = [word_feats(tweet) for tweet in tweets_lst]\n",
    "    score = 0\n",
    "    for tweet in test_features:\n",
    "        if classifier.classify(tweet) =='pos':\n",
    "            score += 1 # since most neutral tweets are considered negative\n",
    "        else:\n",
    "            score -= 1\n",
    "    return score/df.shape[0]\n",
    "\n",
    "def location_to_state_abb(location):\n",
    "    abbreviation = location[-2:]\n",
    "    if abbreviation == 'SA':\n",
    "        try:\n",
    "            abbreviation = transform_dict[location]\n",
    "        except:\n",
    "            pass\n",
    "    return abbreviation\n",
    "            \n",
    "transform_dict = {'New York, USA': 'NY', 'New Jersey, USA': 'NJ', 'Pennsylvania, USA': 'PA', 'Georgia, USA': 'GA','Iowa, USA': 'IA', 'Virginia, USA': 'VA', 'Missouri, USA':'MO', 'Alabama, USA': 'AL','North Carolina, USA': 'NC', 'Illinois, USA': 'IL', 'Texas, USA': 'TX', 'Florida, USA': 'FL','Maryland, USA': 'MD', 'Kentucky, USA': 'KY', 'Arkansas, USA': 'AR', 'Alaska, USA': 'AK', 'Kansas, USA': 'KS', 'South Carolina, USA': 'SC', 'Louisiana, USA': 'LA', 'Maine, USA': 'ME', 'Michigan, USA': 'MI', 'Wisconsin, USA': 'WI', 'Delaware, USA': 'DE', 'Arizona, USA': 'AZ', 'Mississippi, USA': 'MS', 'Indiana, USA': 'IN', 'Ohio, USA': 'OH'}\n",
    "\n",
    "def candidates_to_states(df, transformation_dict = transform_dict):\n",
    "    \"\"\"\n",
    "    transforms a dataframe to a dictionary of dataframes with the state abbrevation being the key and a dataframe\n",
    "    of tweets from that state as value.\n",
    "    \"\"\"\n",
    "    df['state_abb'] = df['location'].apply(location_to_state_abb)\n",
    "    states_dict = {}\n",
    "    for state in df['state_abb'].unique():\n",
    "        states_dict[state] = df[df['state_abb'] == state]\n",
    "    return states_dict\n",
    "\n",
    "def score_per_state(dict_with_dfs):\n",
    "    score_dict = {}\n",
    "    for key, values in temp.items():\n",
    "        score_dict[key] = score_per_selection(temp[key])\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5c184cb15b43d0b1ad9c3c017ddf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1d697c0d714542a317ab56db8473f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 4) (2785, 4) (526, 4)\n"
     ]
    }
   ],
   "source": [
    "df = loadTweets(length=5000)\n",
    "both, trump, hillary = tweets_to_candidates(df)\n",
    "print(both.shape, trump.shape, hillary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data set, it is clearly seen that all data can be classified as being on both candidates or just on Trump. The Hillary df has no entries en the else clause does not print anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.725\n",
      "-0.6552962298025135\n",
      "-0.6577946768060836\n"
     ]
    }
   ],
   "source": [
    "print(score_per_selection(both))\n",
    "print(score_per_selection(trump))\n",
    "print(score_per_selection(hillary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.76\n",
      "###########\n",
      "-0.88\n"
     ]
    }
   ],
   "source": [
    "trump_per_state = score_per_state(candidates_to_states(trump))\n",
    "hillary_per_state = score_per_state(candidates_to_states(hillary))\n",
    "both_per_state = score_per_state(candidates_to_states(both))\n",
    "\n",
    "#################### TEST ##################\n",
    "trump_test = candidates_to_states(trump)['NY'].head(50)\n",
    "print(score_per_selection(trump_test))\n",
    "print('###########')\n",
    "hillary_test = candidates_to_states(hillary)['NY'].head(50)\n",
    "print(score_per_selection(hillary_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from descartes import PolygonPatch\n",
    "from matplotlib import cm\n",
    "import math\n",
    "\n",
    "def intensity_creator(dct):\n",
    "    import operator\n",
    "    temp = max(dct.items(), key=operator.itemgetter(1))[0]\n",
    "    return dct[temp]\n",
    "        \n",
    "\n",
    "def create_map(dct, colormap='YlOrRd'):\n",
    "    S_DIR = 'states_21basic/' \n",
    "    BLUE = '#5599ff'\n",
    "    cmap = plt.get_cmap(colormap) # Choose a colormap to be used to color\n",
    "\n",
    "    with open(os.path.join(S_DIR, 'states.json')) as rf:    \n",
    "        data = json.load(rf)\n",
    "\n",
    "    fig = plt.figure() \n",
    "    ax = fig.gca()\n",
    "    for feature in data['features']:\n",
    "        geometry = feature['geometry']\n",
    "\n",
    "        # Convert population to a proportion of the maximum value\n",
    "        name = feature['properties']['STATE_ABBR']\n",
    "        try:\n",
    "            intensity = math.sqrt(dct[name] + 1 / 2)\n",
    "        except:\n",
    "            intensity = 0\n",
    "\n",
    "        if geometry['type'] == 'Polygon':\n",
    "            poly = geometry\n",
    "            ppatch = PolygonPatch(poly, fc=cmap(intensity), ec=BLUE,  alpha=1, zorder=2)\n",
    "            ax.add_patch(ppatch)\n",
    "\n",
    "        elif geometry['type'] == 'MultiPolygon':\n",
    "            for polygon in geometry['coordinates'][0]:\n",
    "                poly = Polygon(polygon)\n",
    "                ppatch = PolygonPatch(poly, fc=cmap(intensity), ec=BLUE, alpha=1, zorder=2)\n",
    "                ax.add_patch(ppatch)\n",
    "        else:\n",
    "            print('Don\\'t know how to draw :', geometry['type'])\n",
    "\n",
    "    ax.axis('scaled')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
